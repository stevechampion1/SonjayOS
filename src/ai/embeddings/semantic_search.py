#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SonjayOS - è¯­ä¹‰æœç´¢æ¨¡å—
åŸºäºHugging FaceåµŒå…¥æ¨¡å‹å®ç°æ–‡ä»¶è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½åˆ†ç±»
"""

import asyncio
import json
import logging
import numpy as np
import sqlite3
import time
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import hashlib
import pickle

try:
    from sentence_transformers import SentenceTransformer
    import torch
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("è­¦å‘Š: sentence-transformersæœªå®‰è£…ï¼Œè¯­ä¹‰æœç´¢åŠŸèƒ½å°†ä¸å¯ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    file_path: str
    similarity_score: float
    content_snippet: str
    file_type: str
    last_modified: float
    file_size: int
    metadata: Dict[str, Any]

@dataclass
class EmbeddingConfig:
    """åµŒå…¥é…ç½®"""
    model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    batch_size: int = 32
    max_length: int = 512
    device: str = "auto"
    cache_size: int = 10000

class SemanticSearch:
    """è¯­ä¹‰æœç´¢ç±»"""
    
    def __init__(self, config_path: str = "/etc/sonjayos/ai/embeddings_config.json"):
        self.config_path = config_path
        self.config = EmbeddingConfig()
        self.model = None
        self.db_path = "/var/lib/sonjayos/embeddings.db"
        self.cache = {}
        self.index_stats = {
            "total_files": 0,
            "total_embeddings": 0,
            "last_index_time": 0,
            "index_duration": 0
        }
        
        self._load_config()
        self._init_database()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "model_name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "batch_size": 32,
            "max_length": 512,
            "device": "auto",
            "cache_size": 10000,
            "supported_extensions": [".txt", ".md", ".py", ".js", ".html", ".css", ".json", ".xml"],
            "chunk_size": 1000,
            "overlap_size": 200
        }
        
        if Path(self.config_path).exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    for key, value in user_config.items():
                        if hasattr(self.config, key):
                            setattr(self.config, key, value)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºæ–‡ä»¶è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT UNIQUE NOT NULL,
                    file_hash TEXT NOT NULL,
                    file_type TEXT NOT NULL,
                    file_size INTEGER NOT NULL,
                    last_modified REAL NOT NULL,
                    content_hash TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºåµŒå…¥è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS embeddings (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER NOT NULL,
                    chunk_index INTEGER NOT NULL,
                    content TEXT NOT NULL,
                    embedding BLOB NOT NULL,
                    FOREIGN KEY (file_id) REFERENCES files (id)
                )
            ''')
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_file_path ON files (file_path)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_file_hash ON files (file_hash)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_embeddings_file_id ON embeddings (file_id)')
            
            conn.commit()
            conn.close()
            
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯­ä¹‰æœç´¢æœåŠ¡"""
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            logger.error("sentence-transformersæœªå®‰è£…ï¼Œæ— æ³•åˆå§‹åŒ–è¯­ä¹‰æœç´¢")
            return False
        
        try:
            # ç¡®å®šè®¾å¤‡
            device = "cuda" if torch.cuda.is_available() else "cpu"
            if self.config.device == "auto":
                self.config.device = device
            
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.config.device}")
            
            # åŠ è½½æ¨¡å‹
            logger.info(f"åŠ è½½åµŒå…¥æ¨¡å‹: {self.config.model_name}")
            self.model = SentenceTransformer(
                self.config.model_name,
                device=self.config.device
            )
            
            # åŠ è½½ç»Ÿè®¡ä¿¡æ¯
            self._load_stats()
            
            logger.info("è¯­ä¹‰æœç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _load_stats(self):
        """åŠ è½½ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è·å–æ–‡ä»¶æ€»æ•°
            cursor.execute('SELECT COUNT(*) FROM files')
            self.index_stats["total_files"] = cursor.fetchone()[0]
            
            # è·å–åµŒå…¥æ€»æ•°
            cursor.execute('SELECT COUNT(*) FROM embeddings')
            self.index_stats["total_embeddings"] = cursor.fetchone()[0]
            
            # è·å–æœ€åç´¢å¼•æ—¶é—´
            cursor.execute('SELECT MAX(created_at) FROM files')
            result = cursor.fetchone()[0]
            if result:
                self.index_stats["last_index_time"] = result
            
            conn.close()
            
        except Exception as e:
            logger.warning(f"åŠ è½½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    async def index_directory(self, directory_path: str, recursive: bool = True) -> Dict[str, Any]:
        """ç´¢å¼•ç›®å½•ä¸­çš„æ–‡ä»¶"""
        start_time = time.time()
        indexed_files = 0
        skipped_files = 0
        errors = []
        
        try:
            directory = Path(directory_path)
            if not directory.exists():
                raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
            
            # è·å–æ‰€æœ‰æ–‡ä»¶
            if recursive:
                files = list(directory.rglob("*"))
            else:
                files = list(directory.iterdir())
            
            # è¿‡æ»¤æ–‡ä»¶
            supported_extensions = [".txt", ".md", ".py", ".js", ".html", ".css", ".json", ".xml"]
            files = [f for f in files if f.is_file() and f.suffix.lower() in supported_extensions]
            
            logger.info(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶éœ€è¦ç´¢å¼•")
            
            # æ‰¹é‡å¤„ç†æ–‡ä»¶
            for file_path in files:
                try:
                    if await self._index_file(file_path):
                        indexed_files += 1
                    else:
                        skipped_files += 1
                        
                except Exception as e:
                    error_msg = f"ç´¢å¼•æ–‡ä»¶å¤±è´¥ {file_path}: {e}"
                    logger.error(error_msg)
                    errors.append(error_msg)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.index_stats["total_files"] += indexed_files
            self.index_stats["last_index_time"] = time.time()
            self.index_stats["index_duration"] = time.time() - start_time
            
            logger.info(f"ç´¢å¼•å®Œæˆ: {indexed_files} ä¸ªæ–‡ä»¶å·²ç´¢å¼•, {skipped_files} ä¸ªæ–‡ä»¶è·³è¿‡")
            
            return {
                "indexed_files": indexed_files,
                "skipped_files": skipped_files,
                "errors": errors,
                "duration": self.index_stats["index_duration"]
            }
            
        except Exception as e:
            logger.error(f"ç´¢å¼•ç›®å½•å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def _index_file(self, file_path: Path) -> bool:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self._calculate_file_hash(file_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœªæ›´æ”¹
            if self._is_file_unchanged(file_path, file_hash):
                return False
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            content = self._read_file_content(file_path)
            if not content:
                return False
            
            # åˆ†å—å¤„ç†
            chunks = self._split_content(content)
            
            # ç”ŸæˆåµŒå…¥
            embeddings = await self._generate_embeddings(chunks)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            await self._save_file_embeddings(file_path, file_hash, content, chunks, embeddings)
            
            return True
            
        except Exception as e:
            logger.error(f"ç´¢å¼•æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception:
            return ""
    
    def _is_file_unchanged(self, file_path: Path, file_hash: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœªæ›´æ”¹"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute(
                'SELECT file_hash FROM files WHERE file_path = ?',
                (str(file_path),)
            )
            result = cursor.fetchone()
            
            conn.close()
            
            return result and result[0] == file_hash
            
        except Exception:
            return False
    
    def _read_file_content(self, file_path: Path) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue
            
            logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œè·³è¿‡")
            return ""
            
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _split_content(self, content: str) -> List[str]:
        """åˆ†å‰²å†…å®¹ä¸ºå—"""
        chunk_size = 1000
        overlap_size = 200
        
        chunks = []
        start = 0
        
        while start < len(content):
            end = min(start + chunk_size, len(content))
            chunk = content[start:end]
            
            # å°è¯•åœ¨å¥å·æˆ–æ¢è¡Œç¬¦å¤„åˆ†å‰²
            if end < len(content):
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                split_point = max(last_period, last_newline)
                
                if split_point > start + chunk_size // 2:
                    chunk = chunk[:split_point + 1]
                    end = start + split_point + 1
            
            chunks.append(chunk.strip())
            start = end - overlap_size
            
        return [chunk for chunk in chunks if chunk.strip()]
    
    async def _generate_embeddings(self, chunks: List[str]) -> List[np.ndarray]:
        """ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            # æ‰¹é‡å¤„ç†
            embeddings = []
            batch_size = self.config.batch_size
            
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch,
                    convert_to_tensor=True,
                    show_progress_bar=False
                )
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                if hasattr(batch_embeddings, 'cpu'):
                    batch_embeddings = batch_embeddings.cpu().numpy()
                
                embeddings.extend(batch_embeddings)
            
            return embeddings
            
        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
            return []
    
    async def _save_file_embeddings(self, file_path: Path, file_hash: str, content: str, 
                                   chunks: List[str], embeddings: List[np.ndarray]):
        """ä¿å­˜æ–‡ä»¶åµŒå…¥åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ é™¤æ—§è®°å½•
            cursor.execute('DELETE FROM files WHERE file_path = ?', (str(file_path),))
            
            # æ’å…¥æ–‡ä»¶è®°å½•
            cursor.execute('''
                INSERT INTO files (file_path, file_hash, file_type, file_size, last_modified, content_hash)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                str(file_path),
                file_hash,
                file_path.suffix.lower(),
                file_path.stat().st_size,
                file_path.stat().st_mtime,
                hashlib.md5(content.encode()).hexdigest()
            ))
            
            file_id = cursor.lastrowid
            
            # æ’å…¥åµŒå…¥è®°å½•
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                embedding_blob = pickle.dumps(embedding)
                cursor.execute('''
                    INSERT INTO embeddings (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                ''', (file_id, i, chunk, embedding_blob))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"ä¿å­˜åµŒå…¥å¤±è´¥: {e}")
            raise
    
    async def search(self, query: str, limit: int = 10, min_similarity: float = 0.3) -> List[SearchResult]:
        """è¯­ä¹‰æœç´¢"""
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.model.encode([query], convert_to_tensor=True)
            if hasattr(query_embedding, 'cpu'):
                query_embedding = query_embedding.cpu().numpy()[0]
            
            # ä»æ•°æ®åº“è·å–æ‰€æœ‰åµŒå…¥
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT f.file_path, f.file_type, f.file_size, f.last_modified, 
                       e.chunk_index, e.content, e.embedding
                FROM files f
                JOIN embeddings e ON f.id = e.file_id
            ''')
            
            results = []
            
            for row in cursor.fetchall():
                file_path, file_type, file_size, last_modified, chunk_index, content, embedding_blob = row
                
                # ååºåˆ—åŒ–åµŒå…¥
                embedding = pickle.loads(embedding_blob)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = np.dot(query_embedding, embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
                )
                
                if similarity >= min_similarity:
                    results.append(SearchResult(
                        file_path=file_path,
                        similarity_score=float(similarity),
                        content_snippet=content[:200] + "..." if len(content) > 200 else content,
                        file_type=file_type,
                        last_modified=last_modified,
                        file_size=file_size,
                        metadata={"chunk_index": chunk_index}
                    ))
            
            conn.close()
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    async def get_similar_files(self, file_path: str, limit: int = 5) -> List[SearchResult]:
        """è·å–ç›¸ä¼¼æ–‡ä»¶"""
        try:
            # è·å–æ–‡ä»¶çš„åµŒå…¥
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT e.embedding, e.content
                FROM files f
                JOIN embeddings e ON f.id = e.file_id
                WHERE f.file_path = ?
                LIMIT 1
            ''', (file_path,))
            
            result = cursor.fetchone()
            if not result:
                return []
            
            embedding_blob, content = result
            embedding = pickle.loads(embedding_blob)
            
            # æœç´¢ç›¸ä¼¼æ–‡ä»¶
            cursor.execute('''
                SELECT f.file_path, f.file_type, f.file_size, f.last_modified,
                       e.chunk_index, e.content, e.embedding
                FROM files f
                JOIN embeddings e ON f.id = e.file_id
                WHERE f.file_path != ?
            ''', (file_path,))
            
            results = []
            
            for row in cursor.fetchall():
                other_file_path, file_type, file_size, last_modified, chunk_index, other_content, other_embedding_blob = row
                
                other_embedding = pickle.loads(other_embedding_blob)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = np.dot(embedding, other_embedding) / (
                    np.linalg.norm(embedding) * np.linalg.norm(other_embedding)
                )
                
                results.append(SearchResult(
                    file_path=other_file_path,
                    similarity_score=float(similarity),
                    content_snippet=other_content[:200] + "..." if len(other_content) > 200 else other_content,
                    file_type=file_type,
                    last_modified=last_modified,
                    file_size=file_size,
                    metadata={"chunk_index": chunk_index}
                ))
            
            conn.close()
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"è·å–ç›¸ä¼¼æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_loaded": self.model is not None,
            "database_path": self.db_path,
            "index_stats": self.index_stats,
            "cache_size": len(self.cache)
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.cache.clear()
        self.model = None
        logger.info("è¯­ä¹‰æœç´¢æœåŠ¡æ¸…ç†å®Œæˆ")

# å…¨å±€å®ä¾‹
semantic_search = SemanticSearch()

async def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    search = SemanticSearch()
    
    # åˆå§‹åŒ–
    if await search.initialize():
        print("âœ… è¯­ä¹‰æœç´¢æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # ç´¢å¼•æµ‹è¯•ç›®å½•
        test_dir = "/tmp/sonjayos_test"
        Path(test_dir).mkdir(exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_files = [
            ("AIæŠ€æœ¯å‘å±•.md", "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œæ·±åº¦å­¦ä¹ ã€æœºå™¨å­¦ä¹ ç­‰æŠ€æœ¯ä¸æ–­çªç ´ã€‚"),
            ("æ“ä½œç³»ç»Ÿè®¾è®¡.py", "class OperatingSystem:\n    def __init__(self):\n        self.kernel = Kernel()"),
            ("ç”¨æˆ·ç•Œé¢è®¾è®¡.txt", "ç”¨æˆ·ç•Œé¢è®¾è®¡éœ€è¦è€ƒè™‘ç”¨æˆ·ä½“éªŒï¼ŒåŒ…æ‹¬æ˜“ç”¨æ€§ã€ç¾è§‚æ€§å’ŒåŠŸèƒ½æ€§ã€‚")
        ]
        
        for filename, content in test_files:
            with open(Path(test_dir) / filename, 'w', encoding='utf-8') as f:
                f.write(content)
        
        # ç´¢å¼•ç›®å½•
        result = await search.index_directory(test_dir)
        print(f"ğŸ“ ç´¢å¼•ç»“æœ: {result}")
        
        # æµ‹è¯•æœç´¢
        queries = ["äººå·¥æ™ºèƒ½", "æ“ä½œç³»ç»Ÿ", "ç”¨æˆ·ç•Œé¢"]
        
        for query in queries:
            print(f"\nğŸ” æœç´¢: {query}")
            results = await search.search(query, limit=3)
            
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.file_path} (ç›¸ä¼¼åº¦: {result.similarity_score:.3f})")
                print(f"     å†…å®¹: {result.content_snippet}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = search.get_stats()
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {json.dumps(stats, indent=2, ensure_ascii=False)}")
        
        # æ¸…ç†
        search.cleanup()
    else:
        print("âŒ è¯­ä¹‰æœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())
