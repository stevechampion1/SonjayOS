#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SonjayOS - Whisperè¯­éŸ³è¯†åˆ«æ¨¡å—
æä¾›å®æ—¶è¯­éŸ³è½¬æ–‡æœ¬åŠŸèƒ½ï¼Œæ”¯æŒå¤šè¯­è¨€å’Œå™ªå£°ç¯å¢ƒ
"""

import asyncio
import logging
import numpy as np
import pyaudio
import wave
import threading
import queue
import time
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass
from pathlib import Path
import json

try:
    import whisper
    import torch
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("è­¦å‘Š: Whisperæœªå®‰è£…ï¼Œè¯­éŸ³è¯†åˆ«åŠŸèƒ½å°†ä¸å¯ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AudioConfig:
    """éŸ³é¢‘é…ç½®"""
    sample_rate: int = 16000
    channels: int = 1
    chunk_size: int = 1024
    format: int = pyaudio.paInt16
    device_index: Optional[int] = None

@dataclass
class RecognitionConfig:
    """è¯†åˆ«é…ç½®"""
    language: str = "zh"  # ä¸­æ–‡
    model_size: str = "base"  # tiny, base, small, medium, large
    temperature: float = 0.0
    beam_size: int = 5
    best_of: int = 5
    patience: float = 1.0
    length_penalty: float = 1.0
    suppress_tokens: str = "-1"
    initial_prompt: Optional[str] = None
    word_timestamps: bool = False
    condition_on_previous_text: bool = True
    fp16: bool = True
    compression_ratio_threshold: float = 2.4
    logprob_threshold: float = -1.0
    no_speech_threshold: float = 0.6

class SpeechRecognition:
    """è¯­éŸ³è¯†åˆ«ç±»"""
    
    def __init__(self, config_path: str = "/etc/sonjayos/ai/whisper_config.json"):
        self.config_path = config_path
        self.audio_config = AudioConfig()
        self.recognition_config = RecognitionConfig()
        self.model = None
        self.audio = None
        self.stream = None
        self.is_recording = False
        self.audio_queue = queue.Queue()
        self.recognition_queue = queue.Queue()
        self.callbacks = []
        self.recording_thread = None
        self.processing_thread = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_audio_duration": 0.0,
            "total_processing_time": 0.0,
            "recognition_accuracy": 0.0,
            "average_latency": 0.0
        }
        
        self._load_config()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "audio": {
                "sample_rate": 16000,
                "channels": 1,
                "chunk_size": 1024,
                "device_index": None
            },
            "recognition": {
                "language": "zh",
                "model_size": "base",
                "temperature": 0.0,
                "beam_size": 5,
                "word_timestamps": False
            },
            "performance": {
                "max_audio_length": 30,  # ç§’
                "vad_threshold": 0.5,    # è¯­éŸ³æ´»åŠ¨æ£€æµ‹é˜ˆå€¼
                "silence_timeout": 2.0   # é™éŸ³è¶…æ—¶
            }
        }
        
        if Path(self.config_path).exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # æ›´æ–°é…ç½®
                    if "audio" in user_config:
                        for key, value in user_config["audio"].items():
                            setattr(self.audio_config, key, value)
                    if "recognition" in user_config:
                        for key, value in user_config["recognition"].items():
                            setattr(self.recognition_config, key, value)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«æœåŠ¡"""
        if not WHISPER_AVAILABLE:
            logger.error("Whisperæœªå®‰è£…ï¼Œæ— æ³•åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«")
            return False
        
        try:
            # æ£€æŸ¥CUDAå¯ç”¨æ€§
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            # åŠ è½½Whisperæ¨¡å‹
            logger.info(f"åŠ è½½Whisperæ¨¡å‹: {self.recognition_config.model_size}")
            self.model = whisper.load_model(
                self.recognition_config.model_size,
                device=device
            )
            
            # åˆå§‹åŒ–éŸ³é¢‘ç³»ç»Ÿ
            self.audio = pyaudio.PyAudio()
            
            # åˆ—å‡ºå¯ç”¨éŸ³é¢‘è®¾å¤‡
            self._list_audio_devices()
            
            logger.info("è¯­éŸ³è¯†åˆ«æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _list_audio_devices(self):
        """åˆ—å‡ºå¯ç”¨éŸ³é¢‘è®¾å¤‡"""
        logger.info("å¯ç”¨éŸ³é¢‘è®¾å¤‡:")
        for i in range(self.audio.get_device_count()):
            info = self.audio.get_device_info_by_index(i)
            if info['maxInputChannels'] > 0:
                logger.info(f"  {i}: {info['name']} (è¾“å…¥é€šé“: {info['maxInputChannels']})")
    
    def add_callback(self, callback: Callable[[str, Dict[str, Any]], None]):
        """æ·»åŠ è¯†åˆ«ç»“æœå›è°ƒå‡½æ•°"""
        self.callbacks.append(callback)
    
    def start_listening(self) -> bool:
        """å¼€å§‹ç›‘å¬è¯­éŸ³"""
        if self.is_recording:
            logger.warning("å·²ç»åœ¨ç›‘å¬ä¸­")
            return False
        
        try:
            # åˆ›å»ºéŸ³é¢‘æµ
            self.stream = self.audio.open(
                format=self.audio_config.format,
                channels=self.audio_config.channels,
                rate=self.audio_config.sample_rate,
                input=True,
                input_device_index=self.audio_config.device_index,
                frames_per_buffer=self.audio_config.chunk_size,
                stream_callback=self._audio_callback
            )
            
            self.is_recording = True
            
            # å¯åŠ¨å½•éŸ³çº¿ç¨‹
            self.recording_thread = threading.Thread(target=self._recording_loop)
            self.recording_thread.daemon = True
            self.recording_thread.start()
            
            # å¯åŠ¨å¤„ç†çº¿ç¨‹
            self.processing_thread = threading.Thread(target=self._processing_loop)
            self.processing_thread.daemon = True
            self.processing_thread.start()
            
            logger.info("å¼€å§‹è¯­éŸ³ç›‘å¬")
            return True
            
        except Exception as e:
            logger.error(f"å¯åŠ¨ç›‘å¬å¤±è´¥: {e}")
            return False
    
    def stop_listening(self):
        """åœæ­¢ç›‘å¬è¯­éŸ³"""
        if not self.is_recording:
            return
        
        self.is_recording = False
        
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
            self.stream = None
        
        logger.info("åœæ­¢è¯­éŸ³ç›‘å¬")
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°"""
        if self.is_recording:
            self.audio_queue.put(in_data)
        return (in_data, pyaudio.paContinue)
    
    def _recording_loop(self):
        """å½•éŸ³å¾ªç¯"""
        audio_buffer = []
        silence_frames = 0
        max_silence_frames = int(self.audio_config.sample_rate * 2.0 / self.audio_config.chunk_size)
        
        while self.is_recording:
            try:
                # è·å–éŸ³é¢‘æ•°æ®
                if not self.audio_queue.empty():
                    audio_data = self.audio_queue.get(timeout=0.1)
                    audio_buffer.append(audio_data)
                    silence_frames = 0
                else:
                    silence_frames += 1
                
                # æ£€æŸ¥é™éŸ³è¶…æ—¶
                if silence_frames > max_silence_frames and len(audio_buffer) > 0:
                    # å¤„ç†éŸ³é¢‘ç¼“å†²åŒº
                    audio_array = np.frombuffer(b''.join(audio_buffer), dtype=np.int16)
                    self.recognition_queue.put(audio_array)
                    audio_buffer = []
                    silence_frames = 0
                
                # æ£€æŸ¥ç¼“å†²åŒºå¤§å°
                if len(audio_buffer) > 100:  # é˜²æ­¢å†…å­˜æº¢å‡º
                    audio_array = np.frombuffer(b''.join(audio_buffer), dtype=np.int16)
                    self.recognition_queue.put(audio_array)
                    audio_buffer = []
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"å½•éŸ³å¾ªç¯é”™è¯¯: {e}")
                break
    
    def _processing_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.is_recording:
            try:
                if not self.recognition_queue.empty():
                    audio_array = self.recognition_queue.get(timeout=1.0)
                    
                    # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                    if self._detect_speech_activity(audio_array):
                        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                        audio_float = audio_array.astype(np.float32) / 32768.0
                        
                        # è¯†åˆ«è¯­éŸ³
                        start_time = time.time()
                        result = self._recognize_audio(audio_float)
                        processing_time = time.time() - start_time
                        
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        self._update_stats(len(audio_float) / self.audio_config.sample_rate, processing_time)
                        
                        # è°ƒç”¨å›è°ƒå‡½æ•°
                        if result and result.strip():
                            for callback in self.callbacks:
                                try:
                                    callback(result, {
                                        "confidence": getattr(result, 'confidence', 0.0),
                                        "processing_time": processing_time,
                                        "timestamp": time.time()
                                    })
                                except Exception as e:
                                    logger.error(f"å›è°ƒå‡½æ•°é”™è¯¯: {e}")
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"å¤„ç†å¾ªç¯é”™è¯¯: {e}")
                break
    
    def _detect_speech_activity(self, audio_array: np.ndarray) -> bool:
        """è¯­éŸ³æ´»åŠ¨æ£€æµ‹"""
        # è®¡ç®—RMSèƒ½é‡
        rms = np.sqrt(np.mean(audio_array**2))
        
        # ç®€å•çš„é˜ˆå€¼æ£€æµ‹
        threshold = 0.01  # å¯é…ç½®
        return rms > threshold
    
    def _recognize_audio(self, audio_array: np.ndarray) -> str:
        """è¯†åˆ«éŸ³é¢‘"""
        try:
            # é¢„å¤„ç†éŸ³é¢‘
            audio_array = whisper.pad_or_trim(audio_array)
            
            # ç”Ÿæˆmelé¢‘è°±å›¾
            mel = whisper.log_mel_spectrogram(audio_array).to(self.model.device)
            
            # è§£ç 
            options = whisper.DecodingOptions(
                language=self.recognition_config.language,
                temperature=self.recognition_config.temperature,
                beam_size=self.recognition_config.beam_size,
                best_of=self.recognition_config.best_of,
                patience=self.recognition_config.patience,
                length_penalty=self.recognition_config.length_penalty,
                suppress_tokens=self.recognition_config.suppress_tokens,
                initial_prompt=self.recognition_config.initial_prompt,
                condition_on_previous_text=self.recognition_config.condition_on_previous_text,
                fp16=self.recognition_config.fp16,
                compression_ratio_threshold=self.recognition_config.compression_ratio_threshold,
                logprob_threshold=self.recognition_config.logprob_threshold,
                no_speech_threshold=self.recognition_config.no_speech_threshold
            )
            
            result = whisper.decode(self.model, mel, options)
            
            return result.text
            
        except Exception as e:
            logger.error(f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {e}")
            return ""
    
    def _update_stats(self, audio_duration: float, processing_time: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats["total_audio_duration"] += audio_duration
        self.stats["total_processing_time"] += processing_time
        
        # è®¡ç®—å¹³å‡å»¶è¿Ÿ
        if self.stats["total_audio_duration"] > 0:
            self.stats["average_latency"] = (
                self.stats["total_processing_time"] / 
                self.stats["total_audio_duration"]
            )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "is_recording": self.is_recording,
            "model_loaded": self.model is not None,
            "audio_device_count": self.audio.get_device_count() if self.audio else 0,
            "queue_sizes": {
                "audio_queue": self.audio_queue.qsize(),
                "recognition_queue": self.recognition_queue.qsize()
            },
            "performance_stats": self.stats
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_listening()
        
        if self.audio:
            self.audio.terminate()
            self.audio = None
        
        self.model = None
        logger.info("è¯­éŸ³è¯†åˆ«æœåŠ¡æ¸…ç†å®Œæˆ")

# å…¨å±€å®ä¾‹
speech_recognition = SpeechRecognition()

def on_recognition_result(text: str, metadata: Dict[str, Any]):
    """è¯†åˆ«ç»“æœå›è°ƒç¤ºä¾‹"""
    print(f"ğŸ¤ è¯†åˆ«ç»“æœ: {text}")
    print(f"ğŸ“Š å…ƒæ•°æ®: {metadata}")

async def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    recognition = SpeechRecognition()
    
    # åˆå§‹åŒ–
    if await recognition.initialize():
        print("âœ… è¯­éŸ³è¯†åˆ«æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # æ·»åŠ å›è°ƒ
        recognition.add_callback(on_recognition_result)
        
        # å¼€å§‹ç›‘å¬
        if recognition.start_listening():
            print("ğŸ¤ å¼€å§‹è¯­éŸ³ç›‘å¬ï¼Œè¯·è¯´è¯...")
            print("æŒ‰Ctrl+Cåœæ­¢")
            
            try:
                while True:
                    await asyncio.sleep(1)
                    stats = recognition.get_stats()
                    print(f"ğŸ“Š çŠ¶æ€: {stats}")
            except KeyboardInterrupt:
                print("\nâ¹ï¸ åœæ­¢ç›‘å¬")
            finally:
                recognition.cleanup()
        else:
            print("âŒ å¯åŠ¨ç›‘å¬å¤±è´¥")
    else:
        print("âŒ è¯­éŸ³è¯†åˆ«æœåŠ¡åˆå§‹åŒ–å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())
