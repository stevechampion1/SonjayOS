#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SonjayOS - Llama 3.1 æ¨¡å‹é›†æˆæ¨¡å—
æä¾›æœ¬åœ°AIæ¨ç†æœåŠ¡ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€å¤„ç†å’Œç³»ç»Ÿäº¤äº’
"""

import asyncio
import json
import logging
import os
import subprocess
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from pathlib import Path

import requests
import psutil
from pydantic import BaseModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """AIæ¨¡å‹é…ç½®"""
    name: str
    size: str
    memory_usage: int  # MB
    inference_speed: float  # tokens/second
    quality_score: float  # 0-1

class LlamaRequest(BaseModel):
    """Llamaè¯·æ±‚æ¨¡å‹"""
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    stream: bool = False

class LlamaResponse(BaseModel):
    """Llamaå“åº”æ¨¡å‹"""
    response: str
    tokens_used: int
    inference_time: float
    model_used: str

class LlamaIntegration:
    """Llama 3.1 æ¨¡å‹é›†æˆç±»"""
    
    def __init__(self, config_path: str = "/etc/sonjayos/ai/llama_config.json"):
        self.config_path = config_path
        self.config = self._load_config()
        self.current_model = None
        self.ollama_process = None
        self.model_cache = {}
        self.performance_metrics = {
            "total_requests": 0,
            "average_response_time": 0.0,
            "cache_hit_rate": 0.0
        }
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "models": {
                "llama3.1-8b": {
                    "name": "llama3.1:8b",
                    "size": "8B",
                    "memory_usage": 8000,
                    "inference_speed": 50.0,
                    "quality_score": 0.85
                },
                "llama3.1-70b": {
                    "name": "llama3.1:70b", 
                    "size": "70B",
                    "memory_usage": 40000,
                    "inference_speed": 15.0,
                    "quality_score": 0.95
                }
            },
            "default_model": "llama3.1-8b",
            "ollama_host": "http://localhost:11434",
            "max_memory_usage": 0.8,  # 80% of available RAM
            "cache_size": 1000,
            "auto_model_switching": True
        }
        
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
                
        return default_config
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–AIæœåŠ¡"""
        try:
            # æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
            if not await self._check_ollama_status():
                logger.info("å¯åŠ¨OllamaæœåŠ¡...")
                await self._start_ollama()
                
            # åŠ è½½é»˜è®¤æ¨¡å‹
            default_model = self.config["default_model"]
            await self._load_model(default_model)
            
            logger.info("Llamaé›†æˆåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def _check_ollama_status(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            response = requests.get(f"{self.config['ollama_host']}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    async def _start_ollama(self):
        """å¯åŠ¨OllamaæœåŠ¡"""
        try:
            # æ£€æŸ¥ç³»ç»Ÿèµ„æº
            available_memory = psutil.virtual_memory().available / (1024**3)  # GB
            if available_memory < 8:
                logger.warning("å¯ç”¨å†…å­˜ä¸è¶³ï¼Œå»ºè®®è‡³å°‘8GBå†…å­˜")
            
            # å¯åŠ¨Ollamaè¿›ç¨‹
            self.ollama_process = subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            for _ in range(30):  # æœ€å¤šç­‰å¾…30ç§’
                if await self._check_ollama_status():
                    break
                await asyncio.sleep(1)
            else:
                raise Exception("OllamaæœåŠ¡å¯åŠ¨è¶…æ—¶")
                
        except Exception as e:
            logger.error(f"å¯åŠ¨Ollamaå¤±è´¥: {e}")
            raise
    
    async def _load_model(self, model_name: str) -> bool:
        """åŠ è½½æŒ‡å®šçš„AIæ¨¡å‹"""
        try:
            model_config = self.config["models"].get(model_name)
            if not model_config:
                logger.error(f"æœªæ‰¾åˆ°æ¨¡å‹é…ç½®: {model_name}")
                return False
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
            if not self._check_memory_availability(model_config["memory_usage"]):
                logger.warning(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æ¨¡å‹: {model_name}")
                return False
            
            # æ‹‰å–æ¨¡å‹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            await self._pull_model(model_config["name"])
            
            self.current_model = model_name
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    async def _pull_model(self, model_name: str):
        """æ‹‰å–æ¨¡å‹æ–‡ä»¶"""
        try:
            response = requests.post(
                f"{self.config['ollama_host']}/api/pull",
                json={"name": model_name},
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            response.raise_for_status()
            logger.info(f"æ¨¡å‹æ‹‰å–å®Œæˆ: {model_name}")
        except Exception as e:
            logger.error(f"æ‹‰å–æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _check_memory_availability(self, required_memory: int) -> bool:
        """æ£€æŸ¥å†…å­˜å¯ç”¨æ€§"""
        available_memory = psutil.virtual_memory().available / (1024**2)  # MB
        max_usage = psutil.virtual_memory().total * self.config["max_memory_usage"] / (1024**2)
        
        return available_memory >= required_memory and required_memory <= max_usage
    
    async def generate_response(self, request: LlamaRequest) -> LlamaResponse:
        """ç”ŸæˆAIå“åº”"""
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(request)
            if cache_key in self.model_cache:
                cached_response = self.model_cache[cache_key]
                self.performance_metrics["cache_hit_rate"] += 1
                logger.info("ä½¿ç”¨ç¼“å­˜å“åº”")
                return cached_response
            
            # è‡ªåŠ¨æ¨¡å‹åˆ‡æ¢
            if self.config["auto_model_switching"]:
                await self._auto_switch_model(request)
            
            # å‘é€è¯·æ±‚åˆ°Ollama
            response_data = await self._send_ollama_request(request)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            inference_time = time.time() - start_time
            tokens_used = len(response_data["response"].split())
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            response = LlamaResponse(
                response=response_data["response"],
                tokens_used=tokens_used,
                inference_time=inference_time,
                model_used=self.current_model
            )
            
            # æ›´æ–°ç¼“å­˜
            self._update_cache(cache_key, response)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(inference_time)
            
            return response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            raise
    
    async def _auto_switch_model(self, request: LlamaRequest):
        """æ ¹æ®è¯·æ±‚è‡ªåŠ¨åˆ‡æ¢æ¨¡å‹"""
        prompt_length = len(request.prompt.split())
        
        # æ ¹æ®æç¤ºé•¿åº¦å’Œå¤æ‚åº¦é€‰æ‹©æ¨¡å‹
        if prompt_length > 1000 or "å¤æ‚" in request.prompt:
            target_model = "llama3.1-70b"
        else:
            target_model = "llama3.1-8b"
        
        if target_model != self.current_model:
            await self._load_model(target_model)
    
    async def _send_ollama_request(self, request: LlamaRequest) -> Dict[str, Any]:
        """å‘é€è¯·æ±‚åˆ°Ollama API"""
        payload = {
            "model": self.config["models"][self.current_model]["name"],
            "prompt": request.prompt,
            "stream": request.stream,
            "options": {
                "num_predict": request.max_tokens,
                "temperature": request.temperature,
                "top_p": request.top_p
            }
        }
        
        response = requests.post(
            f"{self.config['ollama_host']}/api/generate",
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        return response.json()
    
    def _generate_cache_key(self, request: LlamaRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        content = f"{request.prompt}_{request.max_tokens}_{request.temperature}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _update_cache(self, cache_key: str, response: LlamaResponse):
        """æ›´æ–°ç¼“å­˜"""
        if len(self.model_cache) >= self.config["cache_size"]:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.model_cache))
            del self.model_cache[oldest_key]
        
        self.model_cache[cache_key] = response
    
    def _update_performance_metrics(self, inference_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["total_requests"] += 1
        current_avg = self.performance_metrics["average_response_time"]
        total_requests = self.performance_metrics["total_requests"]
        
        # è®¡ç®—æ–°çš„å¹³å‡å“åº”æ—¶é—´
        new_avg = (current_avg * (total_requests - 1) + inference_time) / total_requests
        self.performance_metrics["average_response_time"] = new_avg
    
    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            "current_model": self.current_model,
            "available_models": list(self.config["models"].keys()),
            "memory_usage": psutil.virtual_memory().percent,
            "cache_size": len(self.model_cache),
            "performance_metrics": self.performance_metrics,
            "ollama_status": await self._check_ollama_status()
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.ollama_process:
            self.ollama_process.terminate()
            self.ollama_process.wait()
        
        self.model_cache.clear()
        logger.info("Llamaé›†æˆæ¸…ç†å®Œæˆ")

# å…¨å±€å®ä¾‹
llama_integration = LlamaIntegration()

async def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    integration = LlamaIntegration()
    
    # åˆå§‹åŒ–
    if await integration.initialize():
        print("âœ… Llamaé›†æˆåˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•è¯·æ±‚
        test_request = LlamaRequest(
            prompt="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹SonjayOSæ“ä½œç³»ç»Ÿçš„ä¸»è¦ç‰¹ç‚¹ã€‚",
            max_tokens=200
        )
        
        response = await integration.generate_response(test_request)
        print(f"ğŸ¤– AIå“åº”: {response.response}")
        print(f"â±ï¸ æ¨ç†æ—¶é—´: {response.inference_time:.2f}ç§’")
        print(f"ğŸ”¢ ä½¿ç”¨ä»¤ç‰Œ: {response.tokens_used}")
        
        # ç³»ç»ŸçŠ¶æ€
        status = await integration.get_system_status()
        print(f"ğŸ“Š ç³»ç»ŸçŠ¶æ€: {json.dumps(status, indent=2, ensure_ascii=False)}")
        
        # æ¸…ç†
        await integration.cleanup()
    else:
        print("âŒ Llamaé›†æˆåˆå§‹åŒ–å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(main())
